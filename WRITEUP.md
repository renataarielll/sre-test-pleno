# ğŸ“„ Relato TÃ©cnico â€“ Linha do Tempo Completa de ImplementaÃ§Ã£o

## Projeto: sre-pleno-app

Este documento descreve cronologicamente, de forma detalhada e transparente, todo o processo de construÃ§Ã£o, correÃ§Ã£o e validaÃ§Ã£o do projeto sre-pleno-app, desde a escolha da aplicaÃ§Ã£o atÃ© a estabilizaÃ§Ã£o do cluster Kubernetes com observabilidade, escalabilidade e seguranÃ§a.

O objetivo foi demonstrar competÃªncias prÃ¡ticas de SRE, incluindo troubleshooting real, tomada de decisÃ£o sob restriÃ§Ãµes de hardware e entendimento profundo de Kubernetes, Docker e observabilidade.

## ğŸ•’ T-0 â€” Escolha da AplicaÃ§Ã£o e Design Inicial

### Objetivo do Teste

### Construir uma aplicaÃ§Ã£o containerizada capaz de:

* Expor health check
* Expor mÃ©tricas Prometheus
* Gerar logs estruturados
* Escalar automaticamente em Kubernetes
* Integrar com stack de observabilidade (Prometheus + ELK)

### Escolha TecnolÃ³gica

* **Linguagem:** Python
* **Framework:** Flask (simplicidade e previsibilidade)
* **Servidor:** WSGI nativo
* **MÃ©tricas:** prometheus-client
* **Logs:** logging nativo com saÃ­da para arquivo e stdout

### Estrutura Final da AplicaÃ§Ã£o

```
app/
 â”œâ”€â”€ main.py
 â”œâ”€â”€ requirements.txt
``` 

### DecisÃ£o Importante â€“ Logs

* Inicialmente logs seriam apenas em stdout
* DecisÃ£o SRE: escrever tambÃ©m em arquivo local
* Motivo: permitir Filebeat sidecar/DaemonSet coletar logs sem depender exclusivamente do runtime

**ğŸ“Œ Caminho definido:**
```
/app/logs/app.log
```

## ğŸ•’ T-1 â€” ImplementaÃ§Ã£o da AplicaÃ§Ã£o (main.py)

### Funcionalidades Implementadas

* ```/ â†’ endpoint principal```

* ```/health â†’ health check```

* ```/metrics â†’ mÃ©tricas Prometheus```

* ```Logs em arquivo e stdout```

* ```MÃ©trica de contagem e latÃªncia por request```

### CÃ³digo Final (resumo conceitual)

* CriaÃ§Ã£o automÃ¡tica do diretÃ³rio ```/app/logs```

* Logs INFO para:

    * start da aplicaÃ§Ã£o

    * acessos ao ```/health```

* DispatcherMiddleware para ```/metrics```

**ğŸ“Œ ConfirmaÃ§Ã£o prÃ¡tica:**

```
kubectl exec -it <pod> -- sh
ls /app/logs
cat /app/logs/app.log
```

## ğŸ•’ T-2 â€” ContainerizaÃ§Ã£o (Docker)

### Objetivo

Criar uma imagem:
* ImutÃ¡vel
* Simples
* CompatÃ­vel com Kubernetes local (Minikube)

### DecisÃµes

* Base: ```python:3.11-slim```
* ExecuÃ§Ã£o como usuÃ¡rio nÃ£o-root
* Porta exposta: ```8080```
* DiretÃ³rio de trabalho: ```/app```

### Build da imagem
```
docker build -t sre-pleno-app:v4 .
```

### Problema Real Encontrado

Pods rodavam como root, mesmo apÃ³s criar usuÃ¡rio no Dockerfile.

### DiagnÃ³stico

O Minikube estava reutilizando imagem em cache.

### CorreÃ§Ã£o
```
minikube image rm sre-pleno-app:v3
minikube image load sre-pleno-app:v4
kubectl rollout restart deployment sre-pleno-app -n sre-app
```

**ğŸ“Œ Resultado:**
```
kubectl exec -it <pod> -- id
uid=1000(appuser)
```

## ğŸ•’ T-3 â€” Kubernetes Deployment

### Namespace
```
kubectl create namespace sre-app
```

### Deployment

* RÃ©plicas iniciais: 2
* Probes:
    * ```livenessProbe: /health```
    * ```readinessProbe: /health```
* Resources (essencial para HPA):
```
requests:
  cpu: 100m
  memory: 128Mi
limits:
  cpu: 200m
  memory: 256Mi
```

### Erro Encontrado
Pods em ```CrashLoopBackOff```
### Causa
Probe apontava para endpoint inexistente ```(/ready)```
### CorreÃ§Ã£o
AlteraÃ§Ã£o para ```/health```

## ğŸ•’ T-4 â€” Service e Acesso
### Service ClusterIP

* Porta externa: 80
* Target: 8080
```
kubectl port-forward svc/sre-pleno-app-service 8081:80 -n sre-app
curl http://localhost:8081/health
```

## ğŸ•’ T-5 â€” MÃ©tricas Prometheus
### DecisÃ£o
Usar annotations, evitando ServiceMonitor (menos complexidade)
```
annotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"
```
### ValidaÃ§Ã£o
```
kubectl get pods -n sre-app -o jsonpath='{.items[*].metadata.annotations}'
```
## ğŸ•’ T-6 â€” HPA (Escalabilidade)
### ImplementaÃ§Ã£o
* CPU > 70%
* MemÃ³ria > 75%
* MÃ­nimo: 2 pods
* MÃ¡ximo: 5 pods

### Erros Reais Encontrados

1. HPA criado no namespace default
2. MÃ©tricas apareciam como <unknown>

### CorreÃ§Ãµes

* AdiÃ§Ã£o explÃ­cita de namespace: sre-app
* InclusÃ£o de resources.requests

### ValidaÃ§Ã£o
```
kubectl get hpa -n sre-app
```

## ğŸ•’ T-7 â€” Teste de Carga
### ExecuÃ§Ã£o
```
kubectl run load-generator \
  --rm -it \
  --image=busybox \
  -n sre-app -- \
  sh -c "while true; do wget -q -O- http://sre-pleno-app-service; done"
```
### Resultado
* Escala automÃ¡tica para atÃ© 5 pods
* Logs intensificados
* MÃ©tricas refletidas corretamente

## ğŸ•’ T-8 â€” Stack ELK (Observabilidade de Logs)
### DecisÃ£o CrÃ­tica â€“ LimitaÃ§Ã£o de Hardware

O ambiente local nÃ£o suportava:
* Elasticsearch (2Gi)
* Kibana (1Gi)
* Prometheus
* AplicaÃ§Ã£o
* Minikube

### Ajustes NecessÃ¡rios
* ReduÃ§Ã£o do Minikube:
```
minikube start --memory=4096 --cpus=2
```
### Impacto

â— Dashboards prontos do Kibana nÃ£o puderam ser criados, pois:
* Kibana frequentemente entrava em Pending ou Timeout
* Consumo de memÃ³ria tornava o cluster instÃ¡vel

**ğŸ“Œ DecisÃ£o consciente documentada: priorizar ingestÃ£o e visibilidade de logs em vez de dashboards visuais.**

## ğŸ•’ T-9 â€” Elasticsearch
### ImplementaÃ§Ã£o
* Operador Elastic (ECK)
* 1 nÃ³
* mmap desativado
```
node.store.allow_mmap: false
```
## ğŸ•’ T-10 â€” Kibana
* 1 instÃ¢ncia
* Conectada ao Elasticsearch via elasticsearchRef
* Recursos reduzidos

## ğŸ•’ T-11 â€” Filebeat (Coleta de Logs)
### EstratÃ©gia
* DaemonSet
* Coleta logs de:
```
/var/log/containers/*.log
```
### ConfiguraÃ§Ã£o
* Envio para Logstash (conceitual)
* Enriquecimento com metadata Kubernetes
### Erros Encontrados
* YAML invÃ¡lido (containers duplicados)
* ```containers: Required value```
* Namespace incorreto
### CorreÃ§Ãµes
* Reescrita completa do manifesto
* ValidaÃ§Ã£o com:
```
kubectl apply -f filebeat.yaml --dry-run=client
```
### Estado Final
```
kubectl get pods -n sre-app | grep filebeat
filebeat-xxxxx   1/1 Running
```
### ğŸ” SeguranÃ§a â€” Ajustes Implementados
* ExecuÃ§Ã£o non-root (UID 1000)
* Resource limits definidos
* Namespace isolado
* SuperfÃ­cie mÃ­nima de imagem
* Sem exposiÃ§Ã£o externa desnecessÃ¡ria
* TLS do Elasticsearch mantido (verification_mode none apenas em ambiente local)

### ğŸ”„ CI/CD â€” DecisÃ£o Consciente

#### Por que nÃ£o implementar pipeline completo?
* Escopo local
* Ambiente sem registry privado
* Foco em SRE runtime
#### O que foi entregue
* Estrutura ```/ci```
* Docker build reproduzÃ­vel
* Manifests declarativos
* Projeto pronto para CI GitHub Actions (documentado)

#### ğŸ¤– Uso de IA, DocumentaÃ§Ã£o e Comunidade
Durante o projeto foram utilizados:
* IA como auxÃ­lio de debug, nÃ£o geraÃ§Ã£o cega
* DocumentaÃ§Ã£o oficial:
    * Kubernetes
    * Docker
    * Elastic
* FÃ³runs e issues reais (timeouts, HPA unknown, Filebeat crashes)

**ğŸ“Œ Todas as decisÃµes foram validadas manualmente no cluster.**

#### ğŸ Estado Final do Projeto
| Pilar | Status |
| ----- | ------ |
| AplicaÃ§Ã£o | âœ… EstÃ¡vel |
| Kubernetes | âœ… Funcional |
| HPA | âœ… Escalando |
| MÃ©tricas | âœ… Prometheus |
| Logs | âœ… Filebeat |
| SeguranÃ§a | âœ… Adequada |
| Observabilidade | âœ… Funcional |
| DocumentaÃ§Ã£o | âœ… Completa |

### âœ”ï¸ ConclusÃ£o

Este projeto demonstra capacidade prÃ¡tica de SRE, domÃ­nio de troubleshooting, tomada de decisÃ£o sob restriÃ§Ãµes reais e entendimento profundo de infraestrutura moderna.

Nada foi â€œteÃ³ricoâ€.
Tudo foi **construÃ­do, quebrado, analisado e corrigido.**
